---
layout: post
title: "Superchare R with Spark: Apache's SparkR"
category: Machine Learning
tags: modeling r
year: 2015
month: 09
day: 30
published: true
summary: "See how easy it is to set up a few SparkR clusters and control them from RStudio. In this first installment we'll set up multiple clusters on AWS EC2 and control them via RStudio."
image: SparkR/Spark.png
---
**Resources**
<ul>
<li type="square"><a href="https://www.youtube.com/watch?v=3HuYr6G2Z28&list=UUq4pm1i_VZqxKVVOz5qRBIA&index=1" target='_blank'>YouTube Companion Video</a></li>

</ul>
<BR>
<a href='http://spark.apache.org/' target='_blank'>Spark doesn’t need a introduction, but just in case, it extends Hadoop’s distributed/parallel computing paradigm by distributing tasks using both live memory and disk storage. As of version 1.4, SparkR is included in Apache's Spark build. We'll be using version 1.5 here.

This will be a two-part series, here we'll install <b>SparkR</b> on <b>EC2</b> and fire up a few clusters. In the second part, will do some distributed modeling.

<b>Let's get right to it<b>

In order to approach this from the same vantage point, we’ll use a small EC2 instance to launch our Spark clusters. You will need an amazon aws account and the ability to SSH into AWS (more on this later). 

First, sign into the <a href='http://aws.amazon.com/" target="_blank">AWS Console</a>

<p style="text-align:center">
<img src="../img/posts/SparkR/lAWS.png" alt="logging_on_AWS" style='padding:1px; border:1px solid #021a40;'></p>




