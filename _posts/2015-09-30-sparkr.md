---
layout: post
title: "Superchare R with Spark: Apache's SparkR"
category: Machine Learning
tags: modeling r
year: 2015
month: 09
day: 30
published: true
summary: "See how easy it is to set up a few SparkR clusters and control them from RStudio. In this first installment, we'll set up multiple clusters on AWS EC2 and control them via RStudio."
image: SparkR/Spark.png
---
**Resources**
<ul>
<li type="square"><a href="https://www.youtube.com/watch?v=3HuYr6G2Z28&list=UUq4pm1i_VZqxKVVOz5qRBIA&index=1" target='_blank'>YouTube Companion Video</a></li>

</ul>
<BR>
<a href='http://spark.apache.org/' target='_blank'>Spark</a> doesn’t need an introduction, but just in case, it extends Hadoop’s distributed/parallel computing paradigm by distributing tasks using both live memory and disk storage. As of version 1.4, SparkR is included in Apache's Spark build. We'll be using version 1.5 here.

This will be a two-part series, here we'll install <b>SparkR</b> on <b>EC2</b> and fire up a few clusters. In the second part, will do some distributed modeling.

***Let's get right to it***

In order to approach this from the same vantage point, we’ll use a small EC2 instance to launch our Spark clusters. You will need an amazon aws account and the ability to SSH into AWS (more on this later). 

First, sign into the <a href='http://aws.amazon.com/' target="_blank">AWS Console</a>

<p style="text-align:center">
<img src="../img/posts/SparkR/AWS.png" alt="logging_on_AWS" style='padding:1px; border:1px solid #021a40;'></p>

**Under header Networking, select VPC**

<p style="text-align:center">
<img src="../img/posts/SparkR/vpc.png" alt="VPC" style='padding:1px; border:1px solid #021a40;'></p>

A <b>virtual private cloud (VPC)</b> will determine who and what gets to access the site. We will use the wizard and content ourselves with only on VPC. In an enterprise-level application, you will want at least 4, 2 to be private and run your database, and two to be public and hold your web-serving application. By duplicating the private and public VPCs you can benefit from fail-over and load balancing tools. By keeping things simple, we’ll get our instance working in just a few clicks, seriously!

Start the ``wizard`` 

<p style="text-align:center">
<img src="../img/posts/SparkR/vpc_wizard.png" alt="VPC Wizard" style='padding:1px; border:1px solid #021a40;'></p>

Start the wizard and select ``VPC with a Single Public Subnet``: 

<p style="text-align:center">
<img src="../img/posts/SparkR/vpc_configuration.png" alt="VPC Configuration" style='padding:1px; border:1px solid #021a40;'></p>

Most of the defaults are fine except you should add a name under ``VPC name``: 

<img src="../img/posts/SparkR/vpc_single_subnet.png" alt="Single Subnet" style='padding:1px; border:1px solid #021a40;'></p>

***EC2***

VPC is done, let’s now create our EC2 instance - this is going to be our cluster-launching machine. Click on the orange cube in the upper left corner of the page. From the ensuing menu, choose the first option, ``EC2``:

<img src="../img/posts/SparkR/ec2.png" alt="EC2 Icon" style='padding:1px; border:1px solid #021a40;'></p>

In ``Create Instance``, select ``Launch Instance``: 


<img src="../img/posts/SparkR/create_instance.png" alt="Create Instance" style='padding:1px; border:1px solid #021a40;'></p>

Select the first option ``Amazon Linux AMI``:

<img src="../img/posts/SparkR/amazon_ami.png" alt="Create Instance" style='padding:1px; border:1px solid #021a40;'></p>

In **Step 2**, continue with the preselected machine and click ``Next: Configure Instance Details``:

e<img src="../img/posts/SparkR/c2_step2.png" alt="Create Instance" style='padding:1px; border:1px solid #021a40;'></p>

In **Step 3**, keep all defaults but change the ``Auto-assign Public IP`` to ``Enable``:

<img src="../img/posts/SparkR/step_3.png" alt="Create Instance" style='padding:1px; border:1px solid #021a40;'></p>







